[
    [
        "Extracted PDF Text: ayesB\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved Na\u00efve\nWhat is a Ham Email?\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\".\nHAM\nFriendDearLunchMoneyFriendDearLunchMoney\nJunk FolderInbox Folder\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nEmail Keywords Data TableEmailDearFriendLunchMoneyFilter (Class) Sender (A)\u2713\u2713HAMSender (B)\u2713\u2713HAMSender (C)\u2713\u2713\u2713HAMSender (D)\u2713\u2713HAMSender (E)\u2713\u2713HAMSender (F)\u2713\u2713\u2713HAMSender (G)\u2713HAMSender (H)\u2713\u2713HAMSender (I)\u2713SPAMSender (J)\u2713\u2713SPAMSender (K)\u2713\u2713SPAMSender (L)\u2713\u2713SPAMDear Friend Lunch Money \nDear Friend Lunch Money Histogram (HAM) \nHistogram (SPAM) \u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nNa\u00efve Bayes Email Filter (Training)\nConditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome.The probability of a \u201cDear\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.47The probability of a \u201cFriend\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.29The probability of a \u201cLunch\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\ud835\udc34\ud835\udc40=$\"#=0.18The probability of a \u201cMoney\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b\ud835\udc34\ud835\udc40=\"\"#=0.06\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=27=0.29\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=17=0.14\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=07=0\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=47=0.57\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nThese probability calculations are for nominal (categorical) variables, we can also say that these are the Likelihood calculationsNa\u00efve Bayes Email Filter (Prediction)\nQ: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67Dear 0.29 Friend Lunch Money P (\u201cEach Keyword\u201d|S) 0.14 0.00 0.57 \nTraining Spam Samples 4 Money Dear Friend Lunch P (\u201cEach Keyword\u201d|H) 0.47 0.290.18 0.06 Training HAM Samples 8 \ud835\udc43(\ud835\udc46)=48+4=0.33These are called the prior probability \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Prediction)Q: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.67\u00d70.47\u00d70.29\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.09\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.33\u00d70.29\u00d70.14\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.01\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.09>0.01\t\u21d2\u201d\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Serious Problem)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201dis this HAM or SPAM?\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\n\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b4\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.18\u00d70.064\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.000002\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc464\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.00\u00d70.574\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.000002>0.\u21d2\u201d\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Solution)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201d is this HAM or SPAM?\n\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.0012>0.00001.\u21d2\ud835\udc47\u210e\ud835\udc52\t\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52\t\ud835\udc56\ud835\udc60\t\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40Workaround: we add one black box to the count of each word, this is called the \ud835\udec2. In this case, we used \ud835\udec2=1 but this value can be changed. Thus, \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u22600Note: Adding an additional count does not mean adding a new email (instance) to the data, hence the prior probability remains the same. \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46=0+17+4=0.09\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46=4+17+4=0.45\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b=3+117+4=0.19\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b=1+117+4=0.1\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.19\u00d70.14=0.00001Dear Friend Lunch Money Dear Friend Lunch Money HAMSPAM\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.09\u00d70.454=0.0012\ud835\udefc=1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNeighbour LEARNERThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar instances are near to each other.It simply calculates the distance of a new unlabelled data point to all other training data points. There are many ways of calculating distance. However, the straight-line distance (also called the Euclidean distance) is a popular choice..  K-Nearest\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nKNN MECHANISM\nClass 0\nClass 1\nClass 2\nCalculate The Distance (\ud835\udc51\n) \nTake a vote among Nearest K neighbours  k=3k=11k=1\nThere are many ways to evaluate the distance function such as Euclidean, Manhattan and Minkowski functions\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nCHOOSING K VALUE\nWhat is the best value \nof K for a dataset? \n\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n\t\n(\n\ud835\udc5b\n)\n \n\u00d7\n \n\ud835\udc39\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\n(\n\ud835\udc5a\n)\nK = 1\nK = 3\nK = 5\nK =2\nK = 7\nK = 9\nK = 11\nK = \ud835\udc5a\nK =10\nK =4\nK =6\nK =8\nK =13\nK =100\nK =n\nK =m\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nK =?\n\nAs we decrease the value of K to 1, our predictions become less stable.As we increase the value of K, our predictions become more stable due to majority voting and thus, more likely to make more accurate predictions (up to a certain point)Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\u201cChoose an odd value of K, to allow a tie \nbreaker\u201d\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvClassifying unknown records is relatively expensive.vLazy learner; must compute distance over k neighbours.vLarge data sets mean expensive calculationvAccuracy is severely degraded by noisy or irrelevant functions.Disadvantages of k-Nearest Neighbours\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple.Lazy Learner KNN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING\nTESTING\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING AND TESTING \nSUPPLIED \nTEST SET \nK\n-\nFOLD \nCROSS \nVALIDATION\nUSE \nTRAINING \nSET\nTest your model on the same learnt data. Not very accepted because you can just build a model to memorize the training instances (which will be in the test)\nUse an external (new) dataset file to test your trained model. This can be expensive!\nSplits the data and separates x% of the data for training and the rest of it for testing. \nPERCENTAGE \nSPLIT\nFold the data in k-folds and repeat k-times. Then use 9 folds for training and leave 1 fold out for testing. Every time leave a different fold for testing.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTESTING OPTIONS  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\nUse Training Set \nSupplied Test Set \nPercentage Split\nK\n-\nFold Cross Validation\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nIn ML training, the \nparameters of a ML algorithm are adjusted to produce best prediction. This process is knowns as fitting.  In ML, the training process, fits the algorithm\u2019s parameters around the training examples to become a model. The final adjusted parameters define the signal in the training data  \nUse Training Set \nTHE MODEL = ALGORITHM \n+\n \nTRAINING DATA  \nTHE ADJUSTED KNOBS ARE FITTED PARAMETERS   \nTHE ALGORITHM  \nTHE UNADJUSTED KNOBS ARE PARAMETERS   \nPercentage Split\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nFitting the parameters only according to the training dataset could make the model only specific to the training sample pattern which was used to build it.This could result in a problem known as Over Fitting, because other data out there differ from the training data. \nMaking a ML model suitable to predict data points other than training unlocks the model from being specific to the training data, becoming a General model. \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nTHE FINAL ESTIMATED MODEL \nWITH AVERAGED PARAMETERS ADJUSTEMENT    \nOne way to a generalised model is to estimate how well the model will do on other data. The estimation is done by carrying out training and testing on different datasets. In the process the parameters will be adjusted multiple times (Tuned) to estimate a final model with average adjusted parameters  \nMODEL(1)\nDataset 1\nMODEL(2)\nDataset 2\nMODEL(4)\nDataset3\u22ee\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT DOES CROSS\n-\nVALIDATION DO?\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTest\nFINAL AVERAGED MODEL\nTEST ALL RUNTestTestTestTestTestTestTestTestTest\nK=10\nBEFORE TRAINING\nALGORITHM\nMODEL(1)\nFOLD(1) RUN\nMODEL(2)\nFOLD(2) RUN\nMODEL(3)\nFOLD(3) RUN\nMODEL(4)\nFOLD(4) RUN\nMODEL(5)\nFOLD(5) RUN\nMODEL(6)\nFOLD(6) RUN\nMODEL(7)\nFOLD(7) RUN\nMODEL(8)\nFOLD(8) RUN\nMODEL(9)\nFOLD(9) RUN\nMODEL(10)\nFOLD(10) RUN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTEST vs VALIDATION\nVALIDATIONTESTSIMILAR BUT DIFFERENT\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nVALIDATION vs TESTING\nA Validation set is a set of examples used for tuning the parameters of a classifier (estimating best parameters)\nA Test set is a set of examples used only to assess the performance of a fully-fitted classifier (How well will it do on predicting new real-world data)\nSupplied Test Set \nPercentage Split\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETERS?\nHYPERPARAMETERSPARAMETERSTUNING HYPERPARAMETERS?\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nModel\u2019s Parameters and Hyperparameters, What\u2019s the difference? \nModel\u2019s Learnable Parameters or parameters are the configurations of a model which are internal to the model. Hyperparameters are the explicitly specified parameters that control the training process. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.\nLearnable LR algorithm  Parameters 33\u2022Logistic Regression can sometimes suffer from overfitting and poor generalization to new data. Regularization is a technique that can help mitigate these issues and improve the performance of LR models.\u2022Regularization adds a penalty term C. By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely.\u2022The penalty term is a hyperparameter that controls the strength of the regularization. A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model to give complexity more weight at the cost of fitting the data.\n\nNESTED CROSS\n-\nVALIDATION  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nNested Cross\n-\nValidation\nK=10\nK=5Inner F1Inner F2Inner F3Inner F4Inner F5\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nK=10\nK\n-\nFold Cross ValidationOuter F1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETER TUNING\nNested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. i.e., GridSearchCV.\nThe outer cross-validation is performed to estimate predictability of the data. In each iteration, data are divided into training and test sets. Before training, another (inner) cross-validation loop is used to optimize hyperparameters.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n",
        "Extracted PDF Text: ayesB\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved Na\u00efve\nWhat is a Ham Email?\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\".\nHAM\nFriendDearLunchMoneyFriendDearLunchMoney\nJunk FolderInbox Folder\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nEmail Keywords Data TableEmailDearFriendLunchMoneyFilter (Class) Sender (A)\u2713\u2713HAMSender (B)\u2713\u2713HAMSender (C)\u2713\u2713\u2713HAMSender (D)\u2713\u2713HAMSender (E)\u2713\u2713HAMSender (F)\u2713\u2713\u2713HAMSender (G)\u2713HAMSender (H)\u2713\u2713HAMSender (I)\u2713SPAMSender (J)\u2713\u2713SPAMSender (K)\u2713\u2713SPAMSender (L)\u2713\u2713SPAMDear Friend Lunch Money \nDear Friend Lunch Money Histogram (HAM) \nHistogram (SPAM) \u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nNa\u00efve Bayes Email Filter (Training)\nConditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome.The probability of a \u201cDear\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.47The probability of a \u201cFriend\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.29The probability of a \u201cLunch\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\ud835\udc34\ud835\udc40=$\"#=0.18The probability of a \u201cMoney\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b\ud835\udc34\ud835\udc40=\"\"#=0.06\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=27=0.29\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=17=0.14\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=07=0\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=47=0.57\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nThese probability calculations are for nominal (categorical) variables, we can also say that these are the Likelihood calculationsNa\u00efve Bayes Email Filter (Prediction)\nQ: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67Dear 0.29 Friend Lunch Money P (\u201cEach Keyword\u201d|S) 0.14 0.00 0.57 \nTraining Spam Samples 4 Money Dear Friend Lunch P (\u201cEach Keyword\u201d|H) 0.47 0.290.18 0.06 Training HAM Samples 8 \ud835\udc43(\ud835\udc46)=48+4=0.33These are called the prior probability \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Prediction)Q: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.67\u00d70.47\u00d70.29\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.09\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.33\u00d70.29\u00d70.14\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.01\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.09>0.01\t\u21d2\u201d\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Serious Problem)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201dis this HAM or SPAM?\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\n\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b4\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.18\u00d70.064\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.000002\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc464\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.00\u00d70.574\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.000002>0.\u21d2\u201d\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Solution)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201d is this HAM or SPAM?\n\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.0012>0.00001.\u21d2\ud835\udc47\u210e\ud835\udc52\t\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52\t\ud835\udc56\ud835\udc60\t\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40Workaround: we add one black box to the count of each word, this is called the \ud835\udec2. In this case, we used \ud835\udec2=1 but this value can be changed. Thus, \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u22600Note: Adding an additional count does not mean adding a new email (instance) to the data, hence the prior probability remains the same. \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46=0+17+4=0.09\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46=4+17+4=0.45\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b=3+117+4=0.19\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b=1+117+4=0.1\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.19\u00d70.14=0.00001Dear Friend Lunch Money Dear Friend Lunch Money HAMSPAM\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.09\u00d70.454=0.0012\ud835\udefc=1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNeighbour LEARNERThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar instances are near to each other.It simply calculates the distance of a new unlabelled data point to all other training data points. There are many ways of calculating distance. However, the straight-line distance (also called the Euclidean distance) is a popular choice..  K-Nearest\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nKNN MECHANISM\nClass 0\nClass 1\nClass 2\nCalculate The Distance (\ud835\udc51\n) \nTake a vote among Nearest K neighbours  k=3k=11k=1\nThere are many ways to evaluate the distance function such as Euclidean, Manhattan and Minkowski functions\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nCHOOSING K VALUE\nWhat is the best value \nof K for a dataset? \n\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n\t\n(\n\ud835\udc5b\n)\n \n\u00d7\n \n\ud835\udc39\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\n(\n\ud835\udc5a\n)\nK = 1\nK = 3\nK = 5\nK =2\nK = 7\nK = 9\nK = 11\nK = \ud835\udc5a\nK =10\nK =4\nK =6\nK =8\nK =13\nK =100\nK =n\nK =m\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nK =?\n\nAs we decrease the value of K to 1, our predictions become less stable.As we increase the value of K, our predictions become more stable due to majority voting and thus, more likely to make more accurate predictions (up to a certain point)Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\u201cChoose an odd value of K, to allow a tie \nbreaker\u201d\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvClassifying unknown records is relatively expensive.vLazy learner; must compute distance over k neighbours.vLarge data sets mean expensive calculationvAccuracy is severely degraded by noisy or irrelevant functions.Disadvantages of k-Nearest Neighbours\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple.Lazy Learner KNN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING\nTESTING\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING AND TESTING \nSUPPLIED \nTEST SET \nK\n-\nFOLD \nCROSS \nVALIDATION\nUSE \nTRAINING \nSET\nTest your model on the same learnt data. Not very accepted because you can just build a model to memorize the training instances (which will be in the test)\nUse an external (new) dataset file to test your trained model. This can be expensive!\nSplits the data and separates x% of the data for training and the rest of it for testing. \nPERCENTAGE \nSPLIT\nFold the data in k-folds and repeat k-times. Then use 9 folds for training and leave 1 fold out for testing. Every time leave a different fold for testing.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTESTING OPTIONS  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\nUse Training Set \nSupplied Test Set \nPercentage Split\nK\n-\nFold Cross Validation\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nIn ML training, the \nparameters of a ML algorithm are adjusted to produce best prediction. This process is knowns as fitting.  In ML, the training process, fits the algorithm\u2019s parameters around the training examples to become a model. The final adjusted parameters define the signal in the training data  \nUse Training Set \nTHE MODEL = ALGORITHM \n+\n \nTRAINING DATA  \nTHE ADJUSTED KNOBS ARE FITTED PARAMETERS   \nTHE ALGORITHM  \nTHE UNADJUSTED KNOBS ARE PARAMETERS   \nPercentage Split\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nFitting the parameters only according to the training dataset could make the model only specific to the training sample pattern which was used to build it.This could result in a problem known as Over Fitting, because other data out there differ from the training data. \nMaking a ML model suitable to predict data points other than training unlocks the model from being specific to the training data, becoming a General model. \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nTHE FINAL ESTIMATED MODEL \nWITH AVERAGED PARAMETERS ADJUSTEMENT    \nOne way to a generalised model is to estimate how well the model will do on other data. The estimation is done by carrying out training and testing on different datasets. In the process the parameters will be adjusted multiple times (Tuned) to estimate a final model with average adjusted parameters  \nMODEL(1)\nDataset 1\nMODEL(2)\nDataset 2\nMODEL(4)\nDataset3\u22ee\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT DOES CROSS\n-\nVALIDATION DO?\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTest\nFINAL AVERAGED MODEL\nTEST ALL RUNTestTestTestTestTestTestTestTestTest\nK=10\nBEFORE TRAINING\nALGORITHM\nMODEL(1)\nFOLD(1) RUN\nMODEL(2)\nFOLD(2) RUN\nMODEL(3)\nFOLD(3) RUN\nMODEL(4)\nFOLD(4) RUN\nMODEL(5)\nFOLD(5) RUN\nMODEL(6)\nFOLD(6) RUN\nMODEL(7)\nFOLD(7) RUN\nMODEL(8)\nFOLD(8) RUN\nMODEL(9)\nFOLD(9) RUN\nMODEL(10)\nFOLD(10) RUN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTEST vs VALIDATION\nVALIDATIONTESTSIMILAR BUT DIFFERENT\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nVALIDATION vs TESTING\nA Validation set is a set of examples used for tuning the parameters of a classifier (estimating best parameters)\nA Test set is a set of examples used only to assess the performance of a fully-fitted classifier (How well will it do on predicting new real-world data)\nSupplied Test Set \nPercentage Split\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETERS?\nHYPERPARAMETERSPARAMETERSTUNING HYPERPARAMETERS?\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nModel\u2019s Parameters and Hyperparameters, What\u2019s the difference? \nModel\u2019s Learnable Parameters or parameters are the configurations of a model which are internal to the model. Hyperparameters are the explicitly specified parameters that control the training process. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.\nLearnable LR algorithm  Parameters 33\u2022Logistic Regression can sometimes suffer from overfitting and poor generalization to new data. Regularization is a technique that can help mitigate these issues and improve the performance of LR models.\u2022Regularization adds a penalty term C. By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely.\u2022The penalty term is a hyperparameter that controls the strength of the regularization. A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model to give complexity more weight at the cost of fitting the data.\n\nNESTED CROSS\n-\nVALIDATION  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nNested Cross\n-\nValidation\nK=10\nK=5Inner F1Inner F2Inner F3Inner F4Inner F5\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nK=10\nK\n-\nFold Cross ValidationOuter F1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETER TUNING\nNested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. i.e., GridSearchCV.\nThe outer cross-validation is performed to estimate predictability of the data. In each iteration, data are divided into training and test sets. Before training, another (inner) cross-validation loop is used to optimize hyperparameters.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n",
        "Extracted PDF Text: ayesB\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved Na\u00efve\nWhat is a Ham Email?\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\".\nHAM\nFriendDearLunchMoneyFriendDearLunchMoney\nJunk FolderInbox Folder\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nEmail Keywords Data TableEmailDearFriendLunchMoneyFilter (Class) Sender (A)\u2713\u2713HAMSender (B)\u2713\u2713HAMSender (C)\u2713\u2713\u2713HAMSender (D)\u2713\u2713HAMSender (E)\u2713\u2713HAMSender (F)\u2713\u2713\u2713HAMSender (G)\u2713HAMSender (H)\u2713\u2713HAMSender (I)\u2713SPAMSender (J)\u2713\u2713SPAMSender (K)\u2713\u2713SPAMSender (L)\u2713\u2713SPAMDear Friend Lunch Money \nDear Friend Lunch Money Histogram (HAM) \nHistogram (SPAM) \u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nNa\u00efve Bayes Email Filter (Training)\nConditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome.The probability of a \u201cDear\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.47The probability of a \u201cFriend\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.29The probability of a \u201cLunch\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\ud835\udc34\ud835\udc40=$\"#=0.18The probability of a \u201cMoney\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b\ud835\udc34\ud835\udc40=\"\"#=0.06\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=27=0.29\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=17=0.14\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=07=0\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=47=0.57\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nThese probability calculations are for nominal (categorical) variables, we can also say that these are the Likelihood calculationsNa\u00efve Bayes Email Filter (Prediction)\nQ: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67Dear 0.29 Friend Lunch Money P (\u201cEach Keyword\u201d|S) 0.14 0.00 0.57 \nTraining Spam Samples 4 Money Dear Friend Lunch P (\u201cEach Keyword\u201d|H) 0.47 0.290.18 0.06 Training HAM Samples 8 \ud835\udc43(\ud835\udc46)=48+4=0.33These are called the prior probability \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Prediction)Q: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.67\u00d70.47\u00d70.29\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.09\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.33\u00d70.29\u00d70.14\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.01\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.09>0.01\t\u21d2\u201d\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Serious Problem)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201dis this HAM or SPAM?\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\n\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b4\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.18\u00d70.064\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.000002\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc464\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.00\u00d70.574\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.000002>0.\u21d2\u201d\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Solution)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201d is this HAM or SPAM?\n\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.0012>0.00001.\u21d2\ud835\udc47\u210e\ud835\udc52\t\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52\t\ud835\udc56\ud835\udc60\t\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40Workaround: we add one black box to the count of each word, this is called the \ud835\udec2. In this case, we used \ud835\udec2=1 but this value can be changed. Thus, \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u22600Note: Adding an additional count does not mean adding a new email (instance) to the data, hence the prior probability remains the same. \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46=0+17+4=0.09\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46=4+17+4=0.45\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b=3+117+4=0.19\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b=1+117+4=0.1\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.19\u00d70.14=0.00001Dear Friend Lunch Money Dear Friend Lunch Money HAMSPAM\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.09\u00d70.454=0.0012\ud835\udefc=1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNeighbour LEARNERThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar instances are near to each other.It simply calculates the distance of a new unlabelled data point to all other training data points. There are many ways of calculating distance. However, the straight-line distance (also called the Euclidean distance) is a popular choice..  K-Nearest\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nKNN MECHANISM\nClass 0\nClass 1\nClass 2\nCalculate The Distance (\ud835\udc51\n) \nTake a vote among Nearest K neighbours  k=3k=11k=1\nThere are many ways to evaluate the distance function such as Euclidean, Manhattan and Minkowski functions\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nCHOOSING K VALUE\nWhat is the best value \nof K for a dataset? \n\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n\t\n(\n\ud835\udc5b\n)\n \n\u00d7\n \n\ud835\udc39\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\n(\n\ud835\udc5a\n)\nK = 1\nK = 3\nK = 5\nK =2\nK = 7\nK = 9\nK = 11\nK = \ud835\udc5a\nK =10\nK =4\nK =6\nK =8\nK =13\nK =100\nK =n\nK =m\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nK =?\n\nAs we decrease the value of K to 1, our predictions become less stable.As we increase the value of K, our predictions become more stable due to majority voting and thus, more likely to make more accurate predictions (up to a certain point)Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\u201cChoose an odd value of K, to allow a tie \nbreaker\u201d\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvClassifying unknown records is relatively expensive.vLazy learner; must compute distance over k neighbours.vLarge data sets mean expensive calculationvAccuracy is severely degraded by noisy or irrelevant functions.Disadvantages of k-Nearest Neighbours\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple.Lazy Learner KNN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING\nTESTING\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING AND TESTING \nSUPPLIED \nTEST SET \nK\n-\nFOLD \nCROSS \nVALIDATION\nUSE \nTRAINING \nSET\nTest your model on the same learnt data. Not very accepted because you can just build a model to memorize the training instances (which will be in the test)\nUse an external (new) dataset file to test your trained model. This can be expensive!\nSplits the data and separates x% of the data for training and the rest of it for testing. \nPERCENTAGE \nSPLIT\nFold the data in k-folds and repeat k-times. Then use 9 folds for training and leave 1 fold out for testing. Every time leave a different fold for testing.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTESTING OPTIONS  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\nUse Training Set \nSupplied Test Set \nPercentage Split\nK\n-\nFold Cross Validation\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nIn ML training, the \nparameters of a ML algorithm are adjusted to produce best prediction. This process is knowns as fitting.  In ML, the training process, fits the algorithm\u2019s parameters around the training examples to become a model. The final adjusted parameters define the signal in the training data  \nUse Training Set \nTHE MODEL = ALGORITHM \n+\n \nTRAINING DATA  \nTHE ADJUSTED KNOBS ARE FITTED PARAMETERS   \nTHE ALGORITHM  \nTHE UNADJUSTED KNOBS ARE PARAMETERS   \nPercentage Split\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nFitting the parameters only according to the training dataset could make the model only specific to the training sample pattern which was used to build it.This could result in a problem known as Over Fitting, because other data out there differ from the training data. \nMaking a ML model suitable to predict data points other than training unlocks the model from being specific to the training data, becoming a General model. \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nTHE FINAL ESTIMATED MODEL \nWITH AVERAGED PARAMETERS ADJUSTEMENT    \nOne way to a generalised model is to estimate how well the model will do on other data. The estimation is done by carrying out training and testing on different datasets. In the process the parameters will be adjusted multiple times (Tuned) to estimate a final model with average adjusted parameters  \nMODEL(1)\nDataset 1\nMODEL(2)\nDataset 2\nMODEL(4)\nDataset3\u22ee\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT DOES CROSS\n-\nVALIDATION DO?\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTest\nFINAL AVERAGED MODEL\nTEST ALL RUNTestTestTestTestTestTestTestTestTest\nK=10\nBEFORE TRAINING\nALGORITHM\nMODEL(1)\nFOLD(1) RUN\nMODEL(2)\nFOLD(2) RUN\nMODEL(3)\nFOLD(3) RUN\nMODEL(4)\nFOLD(4) RUN\nMODEL(5)\nFOLD(5) RUN\nMODEL(6)\nFOLD(6) RUN\nMODEL(7)\nFOLD(7) RUN\nMODEL(8)\nFOLD(8) RUN\nMODEL(9)\nFOLD(9) RUN\nMODEL(10)\nFOLD(10) RUN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTEST vs VALIDATION\nVALIDATIONTESTSIMILAR BUT DIFFERENT\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nVALIDATION vs TESTING\nA Validation set is a set of examples used for tuning the parameters of a classifier (estimating best parameters)\nA Test set is a set of examples used only to assess the performance of a fully-fitted classifier (How well will it do on predicting new real-world data)\nSupplied Test Set \nPercentage Split\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETERS?\nHYPERPARAMETERSPARAMETERSTUNING HYPERPARAMETERS?\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nModel\u2019s Parameters and Hyperparameters, What\u2019s the difference? \nModel\u2019s Learnable Parameters or parameters are the configurations of a model which are internal to the model. Hyperparameters are the explicitly specified parameters that control the training process. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.\nLearnable LR algorithm  Parameters 33\u2022Logistic Regression can sometimes suffer from overfitting and poor generalization to new data. Regularization is a technique that can help mitigate these issues and improve the performance of LR models.\u2022Regularization adds a penalty term C. By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely.\u2022The penalty term is a hyperparameter that controls the strength of the regularization. A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model to give complexity more weight at the cost of fitting the data.\n\nNESTED CROSS\n-\nVALIDATION  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nNested Cross\n-\nValidation\nK=10\nK=5Inner F1Inner F2Inner F3Inner F4Inner F5\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nK=10\nK\n-\nFold Cross ValidationOuter F1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETER TUNING\nNested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. i.e., GridSearchCV.\nThe outer cross-validation is performed to estimate predictability of the data. In each iteration, data are divided into training and test sets. Before training, another (inner) cross-validation loop is used to optimize hyperparameters.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n",
        "You: i need to do a summary of this document",
        "Extracted PDF Text: ayesB\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved Na\u00efve\nWhat is a Ham Email?\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\".\nHAM\nFriendDearLunchMoneyFriendDearLunchMoney\nJunk FolderInbox Folder\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nEmail Keywords Data TableEmailDearFriendLunchMoneyFilter (Class) Sender (A)\u2713\u2713HAMSender (B)\u2713\u2713HAMSender (C)\u2713\u2713\u2713HAMSender (D)\u2713\u2713HAMSender (E)\u2713\u2713HAMSender (F)\u2713\u2713\u2713HAMSender (G)\u2713HAMSender (H)\u2713\u2713HAMSender (I)\u2713SPAMSender (J)\u2713\u2713SPAMSender (K)\u2713\u2713SPAMSender (L)\u2713\u2713SPAMDear Friend Lunch Money \nDear Friend Lunch Money Histogram (HAM) \nHistogram (SPAM) \u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nNa\u00efve Bayes Email Filter (Training)\nConditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome.The probability of a \u201cDear\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.47The probability of a \u201cFriend\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.29The probability of a \u201cLunch\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\ud835\udc34\ud835\udc40=$\"#=0.18The probability of a \u201cMoney\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b\ud835\udc34\ud835\udc40=\"\"#=0.06\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=27=0.29\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=17=0.14\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=07=0\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=47=0.57\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nThese probability calculations are for nominal (categorical) variables, we can also say that these are the Likelihood calculationsNa\u00efve Bayes Email Filter (Prediction)\nQ: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67Dear 0.29 Friend Lunch Money P (\u201cEach Keyword\u201d|S) 0.14 0.00 0.57 \nTraining Spam Samples 4 Money Dear Friend Lunch P (\u201cEach Keyword\u201d|H) 0.47 0.290.18 0.06 Training HAM Samples 8 \ud835\udc43(\ud835\udc46)=48+4=0.33These are called the prior probability \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Prediction)Q: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.67\u00d70.47\u00d70.29\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.09\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.33\u00d70.29\u00d70.14\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.01\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.09>0.01\t\u21d2\u201d\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Serious Problem)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201dis this HAM or SPAM?\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\n\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b4\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.18\u00d70.064\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.000002\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc464\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.00\u00d70.574\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.000002>0.\u21d2\u201d\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Solution)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201d is this HAM or SPAM?\n\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.0012>0.00001.\u21d2\ud835\udc47\u210e\ud835\udc52\t\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52\t\ud835\udc56\ud835\udc60\t\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40Workaround: we add one black box to the count of each word, this is called the \ud835\udec2. In this case, we used \ud835\udec2=1 but this value can be changed. Thus, \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u22600Note: Adding an additional count does not mean adding a new email (instance) to the data, hence the prior probability remains the same. \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46=0+17+4=0.09\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46=4+17+4=0.45\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b=3+117+4=0.19\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b=1+117+4=0.1\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.19\u00d70.14=0.00001Dear Friend Lunch Money Dear Friend Lunch Money HAMSPAM\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.09\u00d70.454=0.0012\ud835\udefc=1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNeighbour LEARNERThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar instances are near to each other.It simply calculates the distance of a new unlabelled data point to all other training data points. There are many ways of calculating distance. However, the straight-line distance (also called the Euclidean distance) is a popular choice..  K-Nearest\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nKNN MECHANISM\nClass 0\nClass 1\nClass 2\nCalculate The Distance (\ud835\udc51\n) \nTake a vote among Nearest K neighbours  k=3k=11k=1\nThere are many ways to evaluate the distance function such as Euclidean, Manhattan and Minkowski functions\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nCHOOSING K VALUE\nWhat is the best value \nof K for a dataset? \n\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n\t\n(\n\ud835\udc5b\n)\n \n\u00d7\n \n\ud835\udc39\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\n(\n\ud835\udc5a\n)\nK = 1\nK = 3\nK = 5\nK =2\nK = 7\nK = 9\nK = 11\nK = \ud835\udc5a\nK =10\nK =4\nK =6\nK =8\nK =13\nK =100\nK =n\nK =m\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nK =?\n\nAs we decrease the value of K to 1, our predictions become less stable.As we increase the value of K, our predictions become more stable due to majority voting and thus, more likely to make more accurate predictions (up to a certain point)Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\u201cChoose an odd value of K, to allow a tie \nbreaker\u201d\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvClassifying unknown records is relatively expensive.vLazy learner; must compute distance over k neighbours.vLarge data sets mean expensive calculationvAccuracy is severely degraded by noisy or irrelevant functions.Disadvantages of k-Nearest Neighbours\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple.Lazy Learner KNN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING\nTESTING\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING AND TESTING \nSUPPLIED \nTEST SET \nK\n-\nFOLD \nCROSS \nVALIDATION\nUSE \nTRAINING \nSET\nTest your model on the same learnt data. Not very accepted because you can just build a model to memorize the training instances (which will be in the test)\nUse an external (new) dataset file to test your trained model. This can be expensive!\nSplits the data and separates x% of the data for training and the rest of it for testing. \nPERCENTAGE \nSPLIT\nFold the data in k-folds and repeat k-times. Then use 9 folds for training and leave 1 fold out for testing. Every time leave a different fold for testing.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTESTING OPTIONS  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\nUse Training Set \nSupplied Test Set \nPercentage Split\nK\n-\nFold Cross Validation\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nIn ML training, the \nparameters of a ML algorithm are adjusted to produce best prediction. This process is knowns as fitting.  In ML, the training process, fits the algorithm\u2019s parameters around the training examples to become a model. The final adjusted parameters define the signal in the training data  \nUse Training Set \nTHE MODEL = ALGORITHM \n+\n \nTRAINING DATA  \nTHE ADJUSTED KNOBS ARE FITTED PARAMETERS   \nTHE ALGORITHM  \nTHE UNADJUSTED KNOBS ARE PARAMETERS   \nPercentage Split\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nFitting the parameters only according to the training dataset could make the model only specific to the training sample pattern which was used to build it.This could result in a problem known as Over Fitting, because other data out there differ from the training data. \nMaking a ML model suitable to predict data points other than training unlocks the model from being specific to the training data, becoming a General model. \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nTHE FINAL ESTIMATED MODEL \nWITH AVERAGED PARAMETERS ADJUSTEMENT    \nOne way to a generalised model is to estimate how well the model will do on other data. The estimation is done by carrying out training and testing on different datasets. In the process the parameters will be adjusted multiple times (Tuned) to estimate a final model with average adjusted parameters  \nMODEL(1)\nDataset 1\nMODEL(2)\nDataset 2\nMODEL(4)\nDataset3\u22ee\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT DOES CROSS\n-\nVALIDATION DO?\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTest\nFINAL AVERAGED MODEL\nTEST ALL RUNTestTestTestTestTestTestTestTestTest\nK=10\nBEFORE TRAINING\nALGORITHM\nMODEL(1)\nFOLD(1) RUN\nMODEL(2)\nFOLD(2) RUN\nMODEL(3)\nFOLD(3) RUN\nMODEL(4)\nFOLD(4) RUN\nMODEL(5)\nFOLD(5) RUN\nMODEL(6)\nFOLD(6) RUN\nMODEL(7)\nFOLD(7) RUN\nMODEL(8)\nFOLD(8) RUN\nMODEL(9)\nFOLD(9) RUN\nMODEL(10)\nFOLD(10) RUN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTEST vs VALIDATION\nVALIDATIONTESTSIMILAR BUT DIFFERENT\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nVALIDATION vs TESTING\nA Validation set is a set of examples used for tuning the parameters of a classifier (estimating best parameters)\nA Test set is a set of examples used only to assess the performance of a fully-fitted classifier (How well will it do on predicting new real-world data)\nSupplied Test Set \nPercentage Split\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETERS?\nHYPERPARAMETERSPARAMETERSTUNING HYPERPARAMETERS?\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nModel\u2019s Parameters and Hyperparameters, What\u2019s the difference? \nModel\u2019s Learnable Parameters or parameters are the configurations of a model which are internal to the model. Hyperparameters are the explicitly specified parameters that control the training process. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.\nLearnable LR algorithm  Parameters 33\u2022Logistic Regression can sometimes suffer from overfitting and poor generalization to new data. Regularization is a technique that can help mitigate these issues and improve the performance of LR models.\u2022Regularization adds a penalty term C. By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely.\u2022The penalty term is a hyperparameter that controls the strength of the regularization. A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model to give complexity more weight at the cost of fitting the data.\n\nNESTED CROSS\n-\nVALIDATION  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nNested Cross\n-\nValidation\nK=10\nK=5Inner F1Inner F2Inner F3Inner F4Inner F5\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nK=10\nK\n-\nFold Cross ValidationOuter F1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETER TUNING\nNested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. i.e., GridSearchCV.\nThe outer cross-validation is performed to estimate predictability of the data. In each iteration, data are divided into training and test sets. Before training, another (inner) cross-validation loop is used to optimize hyperparameters.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n",
        "You: i need to do a summary of this document",
        "Extracted PDF Text: ayesB\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved Na\u00efve\nWhat is a Ham Email?\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\".\nHAM\nFriendDearLunchMoneyFriendDearLunchMoney\nJunk FolderInbox Folder\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nEmail Keywords Data TableEmailDearFriendLunchMoneyFilter (Class) Sender (A)\u2713\u2713HAMSender (B)\u2713\u2713HAMSender (C)\u2713\u2713\u2713HAMSender (D)\u2713\u2713HAMSender (E)\u2713\u2713HAMSender (F)\u2713\u2713\u2713HAMSender (G)\u2713HAMSender (H)\u2713\u2713HAMSender (I)\u2713SPAMSender (J)\u2713\u2713SPAMSender (K)\u2713\u2713SPAMSender (L)\u2713\u2713SPAMDear Friend Lunch Money \nDear Friend Lunch Money Histogram (HAM) \nHistogram (SPAM) \u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nNa\u00efve Bayes Email Filter (Training)\nConditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome.The probability of a \u201cDear\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.47The probability of a \u201cFriend\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.29The probability of a \u201cLunch\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\ud835\udc34\ud835\udc40=$\"#=0.18The probability of a \u201cMoney\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b\ud835\udc34\ud835\udc40=\"\"#=0.06\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=27=0.29\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=17=0.14\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=07=0\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=47=0.57\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nThese probability calculations are for nominal (categorical) variables, we can also say that these are the Likelihood calculationsNa\u00efve Bayes Email Filter (Prediction)\nQ: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67Dear 0.29 Friend Lunch Money P (\u201cEach Keyword\u201d|S) 0.14 0.00 0.57 \nTraining Spam Samples 4 Money Dear Friend Lunch P (\u201cEach Keyword\u201d|H) 0.47 0.290.18 0.06 Training HAM Samples 8 \ud835\udc43(\ud835\udc46)=48+4=0.33These are called the prior probability \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Prediction)Q: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.67\u00d70.47\u00d70.29\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.09\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.33\u00d70.29\u00d70.14\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.01\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.09>0.01\t\u21d2\u201d\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Serious Problem)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201dis this HAM or SPAM?\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\n\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b4\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.18\u00d70.064\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.000002\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc464\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.00\u00d70.574\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.000002>0.\u21d2\u201d\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Solution)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201d is this HAM or SPAM?\n\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.0012>0.00001.\u21d2\ud835\udc47\u210e\ud835\udc52\t\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52\t\ud835\udc56\ud835\udc60\t\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40Workaround: we add one black box to the count of each word, this is called the \ud835\udec2. In this case, we used \ud835\udec2=1 but this value can be changed. Thus, \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u22600Note: Adding an additional count does not mean adding a new email (instance) to the data, hence the prior probability remains the same. \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46=0+17+4=0.09\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46=4+17+4=0.45\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b=3+117+4=0.19\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b=1+117+4=0.1\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.19\u00d70.14=0.00001Dear Friend Lunch Money Dear Friend Lunch Money HAMSPAM\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.09\u00d70.454=0.0012\ud835\udefc=1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNeighbour LEARNERThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar instances are near to each other.It simply calculates the distance of a new unlabelled data point to all other training data points. There are many ways of calculating distance. However, the straight-line distance (also called the Euclidean distance) is a popular choice..  K-Nearest\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nKNN MECHANISM\nClass 0\nClass 1\nClass 2\nCalculate The Distance (\ud835\udc51\n) \nTake a vote among Nearest K neighbours  k=3k=11k=1\nThere are many ways to evaluate the distance function such as Euclidean, Manhattan and Minkowski functions\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nCHOOSING K VALUE\nWhat is the best value \nof K for a dataset? \n\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n\t\n(\n\ud835\udc5b\n)\n \n\u00d7\n \n\ud835\udc39\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\n(\n\ud835\udc5a\n)\nK = 1\nK = 3\nK = 5\nK =2\nK = 7\nK = 9\nK = 11\nK = \ud835\udc5a\nK =10\nK =4\nK =6\nK =8\nK =13\nK =100\nK =n\nK =m\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nK =?\n\nAs we decrease the value of K to 1, our predictions become less stable.As we increase the value of K, our predictions become more stable due to majority voting and thus, more likely to make more accurate predictions (up to a certain point)Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\u201cChoose an odd value of K, to allow a tie \nbreaker\u201d\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvClassifying unknown records is relatively expensive.vLazy learner; must compute distance over k neighbours.vLarge data sets mean expensive calculationvAccuracy is severely degraded by noisy or irrelevant functions.Disadvantages of k-Nearest Neighbours\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple.Lazy Learner KNN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING\nTESTING\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING AND TESTING \nSUPPLIED \nTEST SET \nK\n-\nFOLD \nCROSS \nVALIDATION\nUSE \nTRAINING \nSET\nTest your model on the same learnt data. Not very accepted because you can just build a model to memorize the training instances (which will be in the test)\nUse an external (new) dataset file to test your trained model. This can be expensive!\nSplits the data and separates x% of the data for training and the rest of it for testing. \nPERCENTAGE \nSPLIT\nFold the data in k-folds and repeat k-times. Then use 9 folds for training and leave 1 fold out for testing. Every time leave a different fold for testing.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTESTING OPTIONS  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\nUse Training Set \nSupplied Test Set \nPercentage Split\nK\n-\nFold Cross Validation\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nIn ML training, the \nparameters of a ML algorithm are adjusted to produce best prediction. This process is knowns as fitting.  In ML, the training process, fits the algorithm\u2019s parameters around the training examples to become a model. The final adjusted parameters define the signal in the training data  \nUse Training Set \nTHE MODEL = ALGORITHM \n+\n \nTRAINING DATA  \nTHE ADJUSTED KNOBS ARE FITTED PARAMETERS   \nTHE ALGORITHM  \nTHE UNADJUSTED KNOBS ARE PARAMETERS   \nPercentage Split\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nFitting the parameters only according to the training dataset could make the model only specific to the training sample pattern which was used to build it.This could result in a problem known as Over Fitting, because other data out there differ from the training data. \nMaking a ML model suitable to predict data points other than training unlocks the model from being specific to the training data, becoming a General model. \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nTHE FINAL ESTIMATED MODEL \nWITH AVERAGED PARAMETERS ADJUSTEMENT    \nOne way to a generalised model is to estimate how well the model will do on other data. The estimation is done by carrying out training and testing on different datasets. In the process the parameters will be adjusted multiple times (Tuned) to estimate a final model with average adjusted parameters  \nMODEL(1)\nDataset 1\nMODEL(2)\nDataset 2\nMODEL(4)\nDataset3\u22ee\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT DOES CROSS\n-\nVALIDATION DO?\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTest\nFINAL AVERAGED MODEL\nTEST ALL RUNTestTestTestTestTestTestTestTestTest\nK=10\nBEFORE TRAINING\nALGORITHM\nMODEL(1)\nFOLD(1) RUN\nMODEL(2)\nFOLD(2) RUN\nMODEL(3)\nFOLD(3) RUN\nMODEL(4)\nFOLD(4) RUN\nMODEL(5)\nFOLD(5) RUN\nMODEL(6)\nFOLD(6) RUN\nMODEL(7)\nFOLD(7) RUN\nMODEL(8)\nFOLD(8) RUN\nMODEL(9)\nFOLD(9) RUN\nMODEL(10)\nFOLD(10) RUN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTEST vs VALIDATION\nVALIDATIONTESTSIMILAR BUT DIFFERENT\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nVALIDATION vs TESTING\nA Validation set is a set of examples used for tuning the parameters of a classifier (estimating best parameters)\nA Test set is a set of examples used only to assess the performance of a fully-fitted classifier (How well will it do on predicting new real-world data)\nSupplied Test Set \nPercentage Split\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETERS?\nHYPERPARAMETERSPARAMETERSTUNING HYPERPARAMETERS?\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nModel\u2019s Parameters and Hyperparameters, What\u2019s the difference? \nModel\u2019s Learnable Parameters or parameters are the configurations of a model which are internal to the model. Hyperparameters are the explicitly specified parameters that control the training process. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.\nLearnable LR algorithm  Parameters 33\u2022Logistic Regression can sometimes suffer from overfitting and poor generalization to new data. Regularization is a technique that can help mitigate these issues and improve the performance of LR models.\u2022Regularization adds a penalty term C. By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely.\u2022The penalty term is a hyperparameter that controls the strength of the regularization. A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model to give complexity more weight at the cost of fitting the data.\n\nNESTED CROSS\n-\nVALIDATION  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nNested Cross\n-\nValidation\nK=10\nK=5Inner F1Inner F2Inner F3Inner F4Inner F5\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nK=10\nK\n-\nFold Cross ValidationOuter F1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETER TUNING\nNested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. i.e., GridSearchCV.\nThe outer cross-validation is performed to estimate predictability of the data. In each iteration, data are divided into training and test sets. Before training, another (inner) cross-validation loop is used to optimize hyperparameters.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n"
    ],
    [
        "Extracted PDF Text: ayesB\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved Na\u00efve\nWhat is a Ham Email?\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\".\nHAM\nFriendDearLunchMoneyFriendDearLunchMoney\nJunk FolderInbox Folder\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nEmail Keywords Data TableEmailDearFriendLunchMoneyFilter (Class) Sender (A)\u2713\u2713HAMSender (B)\u2713\u2713HAMSender (C)\u2713\u2713\u2713HAMSender (D)\u2713\u2713HAMSender (E)\u2713\u2713HAMSender (F)\u2713\u2713\u2713HAMSender (G)\u2713HAMSender (H)\u2713\u2713HAMSender (I)\u2713SPAMSender (J)\u2713\u2713SPAMSender (K)\u2713\u2713SPAMSender (L)\u2713\u2713SPAMDear Friend Lunch Money \nDear Friend Lunch Money Histogram (HAM) \nHistogram (SPAM) \u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nNa\u00efve Bayes Email Filter (Training)\nConditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome.The probability of a \u201cDear\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.47The probability of a \u201cFriend\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.29The probability of a \u201cLunch\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\ud835\udc34\ud835\udc40=$\"#=0.18The probability of a \u201cMoney\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b\ud835\udc34\ud835\udc40=\"\"#=0.06\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=27=0.29\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=17=0.14\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=07=0\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=47=0.57\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nThese probability calculations are for nominal (categorical) variables, we can also say that these are the Likelihood calculationsNa\u00efve Bayes Email Filter (Prediction)\nQ: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67Dear 0.29 Friend Lunch Money P (\u201cEach Keyword\u201d|S) 0.14 0.00 0.57 \nTraining Spam Samples 4 Money Dear Friend Lunch P (\u201cEach Keyword\u201d|H) 0.47 0.290.18 0.06 Training HAM Samples 8 \ud835\udc43(\ud835\udc46)=48+4=0.33These are called the prior probability \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Prediction)Q: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.67\u00d70.47\u00d70.29\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.09\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.33\u00d70.29\u00d70.14\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.01\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.09>0.01\t\u21d2\u201d\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Serious Problem)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201dis this HAM or SPAM?\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\n\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b4\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.18\u00d70.064\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.000002\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc464\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.00\u00d70.574\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.000002>0.\u21d2\u201d\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Solution)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201d is this HAM or SPAM?\n\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.0012>0.00001.\u21d2\ud835\udc47\u210e\ud835\udc52\t\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52\t\ud835\udc56\ud835\udc60\t\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40Workaround: we add one black box to the count of each word, this is called the \ud835\udec2. In this case, we used \ud835\udec2=1 but this value can be changed. Thus, \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u22600Note: Adding an additional count does not mean adding a new email (instance) to the data, hence the prior probability remains the same. \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46=0+17+4=0.09\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46=4+17+4=0.45\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b=3+117+4=0.19\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b=1+117+4=0.1\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.19\u00d70.14=0.00001Dear Friend Lunch Money Dear Friend Lunch Money HAMSPAM\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.09\u00d70.454=0.0012\ud835\udefc=1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNeighbour LEARNERThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar instances are near to each other.It simply calculates the distance of a new unlabelled data point to all other training data points. There are many ways of calculating distance. However, the straight-line distance (also called the Euclidean distance) is a popular choice..  K-Nearest\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nKNN MECHANISM\nClass 0\nClass 1\nClass 2\nCalculate The Distance (\ud835\udc51\n) \nTake a vote among Nearest K neighbours  k=3k=11k=1\nThere are many ways to evaluate the distance function such as Euclidean, Manhattan and Minkowski functions\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nCHOOSING K VALUE\nWhat is the best value \nof K for a dataset? \n\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n\t\n(\n\ud835\udc5b\n)\n \n\u00d7\n \n\ud835\udc39\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\n(\n\ud835\udc5a\n)\nK = 1\nK = 3\nK = 5\nK =2\nK = 7\nK = 9\nK = 11\nK = \ud835\udc5a\nK =10\nK =4\nK =6\nK =8\nK =13\nK =100\nK =n\nK =m\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nK =?\n\nAs we decrease the value of K to 1, our predictions become less stable.As we increase the value of K, our predictions become more stable due to majority voting and thus, more likely to make more accurate predictions (up to a certain point)Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\u201cChoose an odd value of K, to allow a tie \nbreaker\u201d\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvClassifying unknown records is relatively expensive.vLazy learner; must compute distance over k neighbours.vLarge data sets mean expensive calculationvAccuracy is severely degraded by noisy or irrelevant functions.Disadvantages of k-Nearest Neighbours\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple.Lazy Learner KNN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING\nTESTING\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING AND TESTING \nSUPPLIED \nTEST SET \nK\n-\nFOLD \nCROSS \nVALIDATION\nUSE \nTRAINING \nSET\nTest your model on the same learnt data. Not very accepted because you can just build a model to memorize the training instances (which will be in the test)\nUse an external (new) dataset file to test your trained model. This can be expensive!\nSplits the data and separates x% of the data for training and the rest of it for testing. \nPERCENTAGE \nSPLIT\nFold the data in k-folds and repeat k-times. Then use 9 folds for training and leave 1 fold out for testing. Every time leave a different fold for testing.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTESTING OPTIONS  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\nUse Training Set \nSupplied Test Set \nPercentage Split\nK\n-\nFold Cross Validation\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nIn ML training, the \nparameters of a ML algorithm are adjusted to produce best prediction. This process is knowns as fitting.  In ML, the training process, fits the algorithm\u2019s parameters around the training examples to become a model. The final adjusted parameters define the signal in the training data  \nUse Training Set \nTHE MODEL = ALGORITHM \n+\n \nTRAINING DATA  \nTHE ADJUSTED KNOBS ARE FITTED PARAMETERS   \nTHE ALGORITHM  \nTHE UNADJUSTED KNOBS ARE PARAMETERS   \nPercentage Split\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nFitting the parameters only according to the training dataset could make the model only specific to the training sample pattern which was used to build it.This could result in a problem known as Over Fitting, because other data out there differ from the training data. \nMaking a ML model suitable to predict data points other than training unlocks the model from being specific to the training data, becoming a General model. \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nTHE FINAL ESTIMATED MODEL \nWITH AVERAGED PARAMETERS ADJUSTEMENT    \nOne way to a generalised model is to estimate how well the model will do on other data. The estimation is done by carrying out training and testing on different datasets. In the process the parameters will be adjusted multiple times (Tuned) to estimate a final model with average adjusted parameters  \nMODEL(1)\nDataset 1\nMODEL(2)\nDataset 2\nMODEL(4)\nDataset3\u22ee\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT DOES CROSS\n-\nVALIDATION DO?\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTest\nFINAL AVERAGED MODEL\nTEST ALL RUNTestTestTestTestTestTestTestTestTest\nK=10\nBEFORE TRAINING\nALGORITHM\nMODEL(1)\nFOLD(1) RUN\nMODEL(2)\nFOLD(2) RUN\nMODEL(3)\nFOLD(3) RUN\nMODEL(4)\nFOLD(4) RUN\nMODEL(5)\nFOLD(5) RUN\nMODEL(6)\nFOLD(6) RUN\nMODEL(7)\nFOLD(7) RUN\nMODEL(8)\nFOLD(8) RUN\nMODEL(9)\nFOLD(9) RUN\nMODEL(10)\nFOLD(10) RUN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTEST vs VALIDATION\nVALIDATIONTESTSIMILAR BUT DIFFERENT\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nVALIDATION vs TESTING\nA Validation set is a set of examples used for tuning the parameters of a classifier (estimating best parameters)\nA Test set is a set of examples used only to assess the performance of a fully-fitted classifier (How well will it do on predicting new real-world data)\nSupplied Test Set \nPercentage Split\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETERS?\nHYPERPARAMETERSPARAMETERSTUNING HYPERPARAMETERS?\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nModel\u2019s Parameters and Hyperparameters, What\u2019s the difference? \nModel\u2019s Learnable Parameters or parameters are the configurations of a model which are internal to the model. Hyperparameters are the explicitly specified parameters that control the training process. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.\nLearnable LR algorithm  Parameters 33\u2022Logistic Regression can sometimes suffer from overfitting and poor generalization to new data. Regularization is a technique that can help mitigate these issues and improve the performance of LR models.\u2022Regularization adds a penalty term C. By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely.\u2022The penalty term is a hyperparameter that controls the strength of the regularization. A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model to give complexity more weight at the cost of fitting the data.\n\nNESTED CROSS\n-\nVALIDATION  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nNested Cross\n-\nValidation\nK=10\nK=5Inner F1Inner F2Inner F3Inner F4Inner F5\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nK=10\nK\n-\nFold Cross ValidationOuter F1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETER TUNING\nNested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. i.e., GridSearchCV.\nThe outer cross-validation is performed to estimate predictability of the data. In each iteration, data are divided into training and test sets. Before training, another (inner) cross-validation loop is used to optimize hyperparameters.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n",
        "You: i need to do a summary of this document",
        "Extracted PDF Text: ayesB\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved Na\u00efve\nWhat is a Ham Email?\"Ham\" is e-mail that is not Spam. In other words, \"non-spam\", or \"good mail\".\nHAM\nFriendDearLunchMoneyFriendDearLunchMoney\nJunk FolderInbox Folder\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nEmail Keywords Data TableEmailDearFriendLunchMoneyFilter (Class) Sender (A)\u2713\u2713HAMSender (B)\u2713\u2713HAMSender (C)\u2713\u2713\u2713HAMSender (D)\u2713\u2713HAMSender (E)\u2713\u2713HAMSender (F)\u2713\u2713\u2713HAMSender (G)\u2713HAMSender (H)\u2713\u2713HAMSender (I)\u2713SPAMSender (J)\u2713\u2713SPAMSender (K)\u2713\u2713SPAMSender (L)\u2713\u2713SPAMDear Friend Lunch Money \nDear Friend Lunch Money Histogram (HAM) \nHistogram (SPAM) \u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nNa\u00efve Bayes Email Filter (Training)\nConditional probability is known as the possibility of an event or outcome happening, based on the existence of a previous event or outcome.The probability of a \u201cDear\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.47The probability of a \u201cFriend\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc34\ud835\udc40=!\"#=0.29The probability of a \u201cLunch\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\ud835\udc34\ud835\udc40=$\"#=0.18The probability of a \u201cMoney\u201d Keyword contained in an email that is HAM is \ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b\ud835\udc34\ud835\udc40=\"\"#=0.06\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=27=0.29\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=17=0.14\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=07=0\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40=47=0.57\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nThese probability calculations are for nominal (categorical) variables, we can also say that these are the Likelihood calculationsNa\u00efve Bayes Email Filter (Prediction)\nQ: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67Dear 0.29 Friend Lunch Money P (\u201cEach Keyword\u201d|S) 0.14 0.00 0.57 \nTraining Spam Samples 4 Money Dear Friend Lunch P (\u201cEach Keyword\u201d|H) 0.47 0.290.18 0.06 Training HAM Samples 8 \ud835\udc43(\ud835\udc46)=48+4=0.33These are called the prior probability \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Prediction)Q: You received an email containing \u201cDear\u201d and \u201cFriend\u201d, is this HAM or SPAM?\ud835\udc43(\ud835\udc3b)=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc3b\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc3b\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.67\u00d70.47\u00d70.29\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc3b\u221d0.09\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc46\u00d7\ud835\udc43\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc46\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.33\u00d70.29\u00d70.14\ud835\udc43\u201c\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\ud835\udc46\u221d0.01\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.09>0.01\t\u21d2\u201d\ud835\udc37\ud835\udc52\ud835\udc4e\ud835\udc5f\t\ud835\udc39\ud835\udc5f\ud835\udc56\ud835\udc52\ud835\udc5b\ud835\udc51\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Serious Problem)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201dis this HAM or SPAM?\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\n\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d\ud835\udc43\ud835\udc3b\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b4\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.18\u00d70.064\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.000002\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d\ud835\udc43\ud835\udc46\u00d7\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u00d7\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc464\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.00\u00d70.574\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.000002>0.\u21d2\u201d\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\u201d\t\ud835\udc56\ud835\udc60\t\ud835\udc3b\ud835\udc34\ud835\udc40\n \u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNa\u00efve Bayes Email Filter (Solution)Q: You received an email containing \u201cLunch\u201d, \u201cMoney\u201d, \u201cMoney\u201d, \u201cMoney\u201d and \u201cMoney\u201d is this HAM or SPAM?\n\ud835\udc43\ud835\udc3b=88+4=0.67\ud835\udc43(\ud835\udc46)=48+4=0.33\ud835\udc46\ud835\udc56\ud835\udc5b\ud835\udc50\ud835\udc52\t0.0012>0.00001.\u21d2\ud835\udc47\u210e\ud835\udc52\t\ud835\udc5a\ud835\udc52\ud835\udc60\ud835\udc60\ud835\udc4e\ud835\udc54\ud835\udc52\t\ud835\udc56\ud835\udc60\t\ud835\udc46\ud835\udc43\ud835\udc34\ud835\udc40Workaround: we add one black box to the count of each word, this is called the \ud835\udec2. In this case, we used \ud835\udec2=1 but this value can be changed. Thus, \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46\u22600Note: Adding an additional count does not mean adding a new email (instance) to the data, hence the prior probability remains the same. \ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc46=0+17+4=0.09\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc46=4+17+4=0.45\ud835\udc43\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\ud835\udc3b=3+117+4=0.19\ud835\udc43\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\ud835\udc3b=1+117+4=0.1\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc3b\u221d0.67\u00d70.19\u00d70.14=0.00001Dear Friend Lunch Money Dear Friend Lunch Money HAMSPAM\ud835\udc43\u201c\ud835\udc3f\ud835\udc62\ud835\udc5b\ud835\udc50\u210e\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\t\ud835\udc40\ud835\udc5c\ud835\udc5b\ud835\udc52\ud835\udc66\u201d\ud835\udc46\u221d0.33\u00d70.09\u00d70.454=0.0012\ud835\udefc=1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nNeighbour LEARNERThe KNN algorithm assumes that similar things exist in close proximity. In other words, similar instances are near to each other.It simply calculates the distance of a new unlabelled data point to all other training data points. There are many ways of calculating distance. However, the straight-line distance (also called the Euclidean distance) is a popular choice..  K-Nearest\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nKNN MECHANISM\nClass 0\nClass 1\nClass 2\nCalculate The Distance (\ud835\udc51\n) \nTake a vote among Nearest K neighbours  k=3k=11k=1\nThere are many ways to evaluate the distance function such as Euclidean, Manhattan and Minkowski functions\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n\nCHOOSING K VALUE\nWhat is the best value \nof K for a dataset? \n\ud835\udc3c\ud835\udc5b\ud835\udc60\ud835\udc61\ud835\udc4e\ud835\udc5b\ud835\udc50\ud835\udc52\ud835\udc60\n\t\n(\n\ud835\udc5b\n)\n \n\u00d7\n \n\ud835\udc39\ud835\udc52\ud835\udc4e\ud835\udc61\ud835\udc62\ud835\udc5f\ud835\udc52\ud835\udc60\n(\n\ud835\udc5a\n)\nK = 1\nK = 3\nK = 5\nK =2\nK = 7\nK = 9\nK = 11\nK = \ud835\udc5a\nK =10\nK =4\nK =6\nK =8\nK =13\nK =100\nK =n\nK =m\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nK =?\n\nAs we decrease the value of K to 1, our predictions become less stable.As we increase the value of K, our predictions become more stable due to majority voting and thus, more likely to make more accurate predictions (up to a certain point)Eventually, we begin to witness an increasing number of errors. It is at this point we know we have pushed the value of K too far.\n\u201cChoose an odd value of K, to allow a tie \nbreaker\u201d\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvClassifying unknown records is relatively expensive.vLazy learner; must compute distance over k neighbours.vLarge data sets mean expensive calculationvAccuracy is severely degraded by noisy or irrelevant functions.Disadvantages of k-Nearest Neighbours\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nvLazy learning (e.g., instance-based learning): Simply stores training data (or only minor processing) and waits until it is given a test tuple.Lazy Learner KNN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING\nTESTING\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTRAINING AND TESTING \nSUPPLIED \nTEST SET \nK\n-\nFOLD \nCROSS \nVALIDATION\nUSE \nTRAINING \nSET\nTest your model on the same learnt data. Not very accepted because you can just build a model to memorize the training instances (which will be in the test)\nUse an external (new) dataset file to test your trained model. This can be expensive!\nSplits the data and separates x% of the data for training and the rest of it for testing. \nPERCENTAGE \nSPLIT\nFold the data in k-folds and repeat k-times. Then use 9 folds for training and leave 1 fold out for testing. Every time leave a different fold for testing.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTESTING OPTIONS  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\nUse Training Set \nSupplied Test Set \nPercentage Split\nK\n-\nFold Cross Validation\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nIn ML training, the \nparameters of a ML algorithm are adjusted to produce best prediction. This process is knowns as fitting.  In ML, the training process, fits the algorithm\u2019s parameters around the training examples to become a model. The final adjusted parameters define the signal in the training data  \nUse Training Set \nTHE MODEL = ALGORITHM \n+\n \nTRAINING DATA  \nTHE ADJUSTED KNOBS ARE FITTED PARAMETERS   \nTHE ALGORITHM  \nTHE UNADJUSTED KNOBS ARE PARAMETERS   \nPercentage Split\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nFitting the parameters only according to the training dataset could make the model only specific to the training sample pattern which was used to build it.This could result in a problem known as Over Fitting, because other data out there differ from the training data. \nMaking a ML model suitable to predict data points other than training unlocks the model from being specific to the training data, becoming a General model. \n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT HAPPENS IN TRAINING?\nTHE FINAL ESTIMATED MODEL \nWITH AVERAGED PARAMETERS ADJUSTEMENT    \nOne way to a generalised model is to estimate how well the model will do on other data. The estimation is done by carrying out training and testing on different datasets. In the process the parameters will be adjusted multiple times (Tuned) to estimate a final model with average adjusted parameters  \nMODEL(1)\nDataset 1\nMODEL(2)\nDataset 2\nMODEL(4)\nDataset3\u22ee\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nWHAT DOES CROSS\n-\nVALIDATION DO?\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTest\nFINAL AVERAGED MODEL\nTEST ALL RUNTestTestTestTestTestTestTestTestTest\nK=10\nBEFORE TRAINING\nALGORITHM\nMODEL(1)\nFOLD(1) RUN\nMODEL(2)\nFOLD(2) RUN\nMODEL(3)\nFOLD(3) RUN\nMODEL(4)\nFOLD(4) RUN\nMODEL(5)\nFOLD(5) RUN\nMODEL(6)\nFOLD(6) RUN\nMODEL(7)\nFOLD(7) RUN\nMODEL(8)\nFOLD(8) RUN\nMODEL(9)\nFOLD(9) RUN\nMODEL(10)\nFOLD(10) RUN\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nTEST vs VALIDATION\nVALIDATIONTESTSIMILAR BUT DIFFERENT\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nVALIDATION vs TESTING\nA Validation set is a set of examples used for tuning the parameters of a classifier (estimating best parameters)\nA Test set is a set of examples used only to assess the performance of a fully-fitted classifier (How well will it do on predicting new real-world data)\nSupplied Test Set \nPercentage Split\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nTestTestTestTestTestTestTestTestTestTest\nK=10\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETERS?\nHYPERPARAMETERSPARAMETERSTUNING HYPERPARAMETERS?\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nModel\u2019s Parameters and Hyperparameters, What\u2019s the difference? \nModel\u2019s Learnable Parameters or parameters are the configurations of a model which are internal to the model. Hyperparameters are the explicitly specified parameters that control the training process. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.\nLearnable LR algorithm  Parameters 33\u2022Logistic Regression can sometimes suffer from overfitting and poor generalization to new data. Regularization is a technique that can help mitigate these issues and improve the performance of LR models.\u2022Regularization adds a penalty term C. By adding a penalty term to the objective function, regularization helps to reduce the complexity of the model and prevent it from fitting the training data too closely.\u2022The penalty term is a hyperparameter that controls the strength of the regularization. A high value of C tells the model to give more weight to the training data. A lower value of C will indicate the model to give complexity more weight at the cost of fitting the data.\n\nNESTED CROSS\n-\nVALIDATION  \nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nNested Cross\n-\nValidation\nK=10\nK=5Inner F1Inner F2Inner F3Inner F4Inner F5\nFold 2\nFold 3\nFold 4\nFold 5\nFold 6\nFold 7\nFold 8\nFold 9\nFold 10\nFold 1\nK=10\nK\n-\nFold Cross ValidationOuter F1\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \nHYPERPARAMETER TUNING\nNested cross-validation (CV) is often used to train a model in which hyperparameters also need to be optimized. Nested CV estimates the generalization error of the underlying model and its (hyper)parameter search. i.e., GridSearchCV.\nThe outer cross-validation is performed to estimate predictability of the data. In each iteration, data are divided into training and test sets. Before training, another (inner) cross-validation loop is used to optimize hyperparameters.\n\u00a9 2023 Mahmoud Aldraimli   All rights reserved \n"
    ]
]